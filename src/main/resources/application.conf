akka {

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = DEBUG
  logger-startup-timeout = 30s
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters = 0
  log-dead-letters-during-shutdown = false

  cluster.jmx.multi-mbeans-in-same-jvm=on

  actor {
    warn-about-java-serializer-usage=off

    provider = cluster

    #allow-java-serialization = off

    serializers {
      tl = "chatter.serializer.ChatTimelineSerializer"
      writer = "chatter.serializer.ChatTimelineWriterSerializer"
      repl = "chatter.serializer.ChatTimelineReplicatorSerializer"
    }

    serialization-bindings {
      "chatter.crdt.ChatTimeline" = tl
      "chatter.actors.typed.WSuccess"  =  writer
      "chatter.actors.typed.WFailure"  =  writer
      "chatter.actors.typed.WTimeout"  =  writer
      "chatter.actors.typed.AskForShards"  =  writer


      "chatter.actors.typed.WriteMessage" = repl
      "chatter.actors.typed.ReadChatTimeline" = repl

      "chatter.actors.typed.RWriteSuccess" = repl
      "chatter.actors.typed.RWriteFailure" = repl
      "chatter.actors.typed.RWriteTimeout" = repl
      "chatter.actors.typed.RChatTimelineReply" = repl
      "chatter.actors.typed.RNotFoundChatTimelineReply" = repl
      "chatter.actors.typed.RGetFailureChatTimelineReply" = repl
    }
  }

  remote {
                          //10262144
    maximum-payload-bytes = 80MiB #30 MB

    netty.tcp {
      hostname = "127.0.0.1"
      message-frame-size =  80MiB
      send-buffer-size =  80MiB
      receive-buffer-size = 80MiB
      maximum-frame-size = 80MiB
    }

    #artery {
    #  advanced {
    #    maximum-frame-size = 80MiB //80

        #The default size of the system messages buffer is 20000
        #system-message-buffer-size = 20000

        # queue for outgoing control (system) messages 
        #outbound-control-queue-size = 3072
    #  }
    #}
  }

  cluster {
    #https://github.com/TanUkkii007/akka-cluster-custom-downing#akka-cluster-custom-downing

    # MajorityLeaderAutoDowning is similar to QuorumLeaderAutoDowning. However, instead of a static specified quorum size
    # this strategy automatically keeps the partition with the largest amount of nodes. If the partitions are of equal size,
    # the partition that contains the node with the globally lowest address is kept. The strategy is the same as the keep majority
    # strategy of Split Brain Resolver from Typesafe reactive platform. If a role is set by majority-member-role,
    # the strategy is only enforced to the nodes with the specified role.
    downing-provider-class = "tanukki.akka.cluster.autodown.MajorityLeaderAutoDowning"

    custom-downing {

      # Time margin after which shards or singletons that belonged to a downed/removed
      # partition are created in surviving partition. The purpose of this margin is that
      # in case of a network partition the persistent actors in the non-surviving partitions
      # must be stopped before corresponding persistent actors are started somewhere else.
      # This is useful if you implement downing strategies that handle network partitions,
      # e.g. by keeping the larger side of the partition and shutting down the smaller side.
      # Decision is taken by the strategy when there has been no membership or
      # reachability changes for this duration, i.e. the cluster state is stable.
      stable-after = 5s

      majority-leader-auto-downing {
        majority-member-role = ""
        down-if-in-minority = true
        shutdown-actor-system-on-resolution = true
      }
    }

    #downing-provider-class = "com.codelfsolutions.cluster.SplitBrainResolver"
    #auto-down-unreachable-after = 5s

    # How many members are needed to start a cluster.
    #min-nr-of-members = 2

    #log-info = off
    failure-detector {
      implementation-class = "akka.remote.PhiAccrualFailureDetector"
      threshold = 10 # 8
      heartbeat-interval = 1 s
      acceptable-heartbeat-pause = 4 s #3
    }

    use-dispatcher = akka.cluster-dispatcher

    metrics.enabled = off
  }

  # CoordinatedShutdown will run the tasks that are added to these
  # phases. The phases can be ordered as a DAG by defining the
  # dependencies between the phases.
  # Each phase is defined as a named config section with the
  # following optional properties:
  # - timeout=15s: Override the default-phase-timeout for this phase.
  # - recover=off: If the phase fails the shutdown is aborted
  #                and depending phases will not be executed.
  # depends-on=[]: Run the phase after the given phases
  coordinated-shutdown {
    # Exit the JVM (System.exit(0)) in the last phase actor-system-terminate
    # if this is set to 'on'. It is done after termination of the
    # ActorSystem if terminate-actor-system=on, otherwise it is done
    # immediately when the last phase is reached.
    exit-jvm = on
    default-phase-timeout = 10 seconds
  }

  rocks-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 2
    }
    throughput = 1000
  }

  cluster-dispatcher {
    type = "Dispatcher"
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 1
      parallelism-max = 2
    }
  }
}